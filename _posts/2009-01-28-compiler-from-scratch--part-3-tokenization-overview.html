---
layout: post
title: "Compiler from Scratch - Part 3: Tokenization Overview"
description: ""
category:
tags: []
permalink: /blog/2009/1/28/compiler-from-scratch-part-3-tokenization-overview.html
---
<p>Here comes part 3 of my infinite part series on developing a compiler from scratch.&#160; See all the posts <a href="/CategoryView,category,Compiler%2BProject.aspx" target="_blank">here</a>.</p>  <p>The first phase of compilation is called Tokenization.&#160; Tokenization is the process of taking individual characters in the source code file and grouping them into more conceptual "Tokens."&#160; We're not parsing the language, or checking syntax, we're just grouping related characters together.&#160; Why do we do this?&#160; If we skipped this step, we'd be passing characters directly to our parser.&#160; The parser doesn't care that the text contains the characters 'f' 'o' 'o', it cares that the source file contains an Identifier called "foo".&#160; There's also a separation of concerns issue.&#160; In theory, if you wanted to allow accented Unicode characters like ?, ?, ?, etc. (you may see blocks or '?' characters instead of the real characters, if so just pretend they're there :D) as valid characters in identifiers, you should only have to change the Tokenizer (to accept those characters and use them, along with the regular alphanumeric characters, to build identifier tokens).&#160; The formal name for Tokenization is actually "Lexical Analysis," and sometimes the Tokens are called "Atoms."</p>  <p>So, how do we Tokenize text?&#160; There are lots of tools out there, most of which let you define your tokens using Regular Expressions (so an identifier would be: "[_A-Za-z][_0-9A-Za-z]*", one letter or underscore followed by zero or more letters, numbers or underscores).&#160; These tools build state machines (aka "<a href="http://en.wikipedia.org/wiki/Finite_state_machine" target="_blank">Finite Automata</a>") that basically walk through all the Regular Expressions until exactly one of them matches the current text buffer.&#160; Tools like "<a href="http://en.wikipedia.org/wiki/Lex_programming_tool" target="_blank">lex</a>" basically allow you to build a full-featured, efficient, Tokenizer by simply writing Regular Expressions.</p>  <p>However, that would be too easy for this series :).&#160; Instead, I'm using a very simple, hand-coded, Tokenizer which walks through the characters, using "if" and "switch" statements to decide what tokens to output.&#160; However, we can't just simply iterate through the characters.&#160; Consider the "-&gt;" operator in Duh (see <a href="/2009/01/19/WritingACompilerFromScratchNdashPart1.aspx" target="_blank">Part 1</a>).&#160; Duh supports the simple arithmetic operators, specifically "-", and I plan to support inequality operators (i.e "&lt;", "&gt;", etc.), so in theory, the "-&gt;" operator could be outputted as a pair of tokens: MINUS and GREATERTHAN (I'll use ALLCAPS to denote the names of tokens).&#160; However, this would be putting a larger burden on the parser to detect the MINUS GREATERTHAN pattern.&#160; Instead, we can take advantage of "lookahead"</p>  <p>The Tokenizer uses the .Net TextReader class, which lets us walk, one character at a time, through the characters in a body of text.&#160; However, once we've called the <code>Read</code> method, the TextReader moves on to the next character and the next time we call it, we'll get the next character.&#160; So, in order to properly parse the "-&gt;" operator, we need a way to look at the next character, without moving the reader.&#160; Fortunately, the TextReader also has the <code>Peek</code> method, which does exactly that.</p>  <p>So now, when we encounter a "-" symbol, we look at the next character.&#160; If it is a "&gt;", we output an ARROW token, and move to the next character.&#160; Otherwise, we output a MINUS token and leave the reader at the "&gt;" character.&#160; Then, in the next iteration, we read the "&gt;" and output a GREATERTHAN token.</p>  <p>Phew, that was a bit long, and more theoretical than I like, but I'll post some code in the next part.&#160; Next time, we'll look at the actual code for the Tokenizer.</p>
