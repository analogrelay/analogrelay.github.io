---
disqus_old: true
layout: post
title: "Compiler from Scratch - Part 3: Tokenization Overview"
description: ""
category:
tags: []
permalink: /blog/2009/1/28/compiler-from-scratch-part-3-tokenization-overview.html
---
<p>Here comes part 3 of my infinite part series on developing a compiler from scratch. See all the posts <a href="/CategoryView,category,Compiler%2BProject.aspx" target="_blank">here</a>.</p>  <p>The first phase of compilation is called Tokenization. Tokenization is the process of taking individual characters in the source code file and grouping them into more conceptual "Tokens." We're not parsing the language, or checking syntax, we're just grouping related characters together. Why do we do this? If we skipped this step, we'd be passing characters directly to our parser. The parser doesn't care that the text contains the characters 'f' 'o' 'o', it cares that the source file contains an Identifier called "foo". There's also a separation of concerns issue. In theory, if you wanted to allow accented Unicode characters like ?, ?, ?, etc. (you may see blocks or '?' characters instead of the real characters, if so just pretend they're there :D) as valid characters in identifiers, you should only have to change the Tokenizer (to accept those characters and use them, along with the regular alphanumeric characters, to build identifier tokens). The formal name for Tokenization is actually "Lexical Analysis," and sometimes the Tokens are called "Atoms."</p>  <p>So, how do we Tokenize text? There are lots of tools out there, most of which let you define your tokens using Regular Expressions (so an identifier would be: "[_A-Za-z][_0-9A-Za-z]*", one letter or underscore followed by zero or more letters, numbers or underscores). These tools build state machines (aka "<a href="http://en.wikipedia.org/wiki/Finite_state_machine" target="_blank">Finite Automata</a>") that basically walk through all the Regular Expressions until exactly one of them matches the current text buffer. Tools like "<a href="http://en.wikipedia.org/wiki/Lex_programming_tool" target="_blank">lex</a>" basically allow you to build a full-featured, efficient, Tokenizer by simply writing Regular Expressions.</p>  <p>However, that would be too easy for this series :). Instead, I'm using a very simple, hand-coded, Tokenizer which walks through the characters, using "if" and "switch" statements to decide what tokens to output. However, we can't just simply iterate through the characters. Consider the "-&gt;" operator in Duh (see <a href="/2009/01/19/WritingACompilerFromScratchNdashPart1.aspx" target="_blank">Part 1</a>). Duh supports the simple arithmetic operators, specifically "-", and I plan to support inequality operators (i.e "&lt;", "&gt;", etc.), so in theory, the "-&gt;" operator could be outputted as a pair of tokens: MINUS and GREATERTHAN (I'll use ALLCAPS to denote the names of tokens). However, this would be putting a larger burden on the parser to detect the MINUS GREATERTHAN pattern. Instead, we can take advantage of "lookahead"</p>  <p>The Tokenizer uses the .Net TextReader class, which lets us walk, one character at a time, through the characters in a body of text. However, once we've called the <code>Read</code> method, the TextReader moves on to the next character and the next time we call it, we'll get the next character. So, in order to properly parse the "-&gt;" operator, we need a way to look at the next character, without moving the reader. Fortunately, the TextReader also has the <code>Peek</code> method, which does exactly that.</p>  <p>So now, when we encounter a "-" symbol, we look at the next character. If it is a "&gt;", we output an ARROW token, and move to the next character. Otherwise, we output a MINUS token and leave the reader at the "&gt;" character. Then, in the next iteration, we read the "&gt;" and output a GREATERTHAN token.</p>  <p>Phew, that was a bit long, and more theoretical than I like, but I'll post some code in the next part. Next time, we'll look at the actual code for the Tokenizer.</p>
